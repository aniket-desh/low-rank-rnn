\documentclass[12pt, twoside]{article}

\usepackage[margin=1in]{geometry}

\usepackage[T1]{fontenc}
\usepackage{cochineal} 
\usepackage{fouriernc}

\usepackage{amsmath}   
\usepackage{amssymb}   
\usepackage{amsfonts}  
\usepackage{enumitem}
\usepackage{bm}

\usepackage{fancyhdr}
\pagestyle{fancy}       % Use the "fancy" page style
\fancyhf{}              % Clear all default header and footer fields
\renewcommand{\headrulewidth}{0pt} % Remove the horizontal line in the header

\fancyhead[LE,RO]{\thepage} % Page number on Left-Even, Right-Odd (outside)
% \fancyhead[RE,LO]{\MakeUppercase{}} % Title on Right-Even, Left-Odd (inside)

\fancypagestyle{plain}{
    \fancyhf{} % Clear header/footer
    \fancyfoot[C]{\thepage} % Center page number in footer
    \renewcommand{\headrulewidth}{0pt}
}

\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\large} % format
  {\thesection.} % label
  {1em} % separation
  {} % code before
\titleformat{\subsection}
  {\normalfont} % format
  {\thesubsection.} % label
  {1em} % separation
  {} % code before

\usepackage{amsthm}
\theoremstyle{plain} 
\newtheorem{theorem}{Theorem}[section] % Numbered like 1.1, 1.2 within sections
\newtheorem{definition}[theorem]{Definition}   % Shares the "theorem" counter
\newtheorem{proposition}[theorem]{Proposition} % Shares the "theorem" counter
\newtheorem{lemma}[theorem]{Lemma}           % Shares the "theorem" counter
\newtheorem{example}[theorem]{Example}         % Shares the "theorem" counter
\newtheorem{exercise}[theorem]{Exercise}       % Shares the "theorem" counter
\newtheorem{remark}[theorem]{Remark}         % Shares the "theorem" counter

\renewcommand{\qedsymbol}{$\Box$} % Set the QED symbol to an open box

\usepackage{hyperref} % For clickable links (TOC, citations, etc.)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Training an RNN with a Low-Rank Structure},
}

\begin{document}

\begin{center}
    {\bfseries Training an RNN with a Low-Rank Structure}\\
    \vspace{1.5em} 
    {\small Aniket Deshpande}
\end{center}
\vspace{1em}

\renewcommand{\contentsname}{\begin{center}\small\MakeUppercase{Contents}\end{center}}
\tableofcontents

\vspace{1cm} % Add some space
\noindent % No indent
Date: \today % Use \today to auto-update, or write a date
\vspace{2mm}\hrule\vspace{2mm} % The horizontal rule
\noindent % No indent
{\small footnote}
\clearpage % Start the main content on a new page


\section{Training an RNN with a Low-Rank Structure}

\textit{Setup}. The model is a continuous time recurrent neural network
\[
\tau \dot{h}(t) = -h(t) + J\phi(h(t)) + B u(t),\quad y(t) = w^T \phi(h(t))
\]
where $J$ is the recurrent weight matrix, $\phi$ is $\mathrm{ReLU}$ or $\tanh$, $u(t)$ is a low-dimensional input (one or two OU stimuli), and $y(t)$ is a 1-2D readout. The task being learned is tracking/denoising the driving OU signals (and possibly switch between two stimuli), i.e. make $y(t)$ match a target trace $y^*(t)$ generated by the OU signals. The loss function would be the mean-squared error over a time window (with light regularization)
\[
\mathcal{L} = \frac{1}{T}\int_0^T \left\| y(t) - y^*(t) \right\|^2 dt + \lambda_W \|J\|_F^2 + \lambda_{\mathrm{bal}} \underbrace{\left\| \frac{1}{N}\mathbf{1}^TJ\right\|_2^2}_{\text{row/col balance}}
\]
The balance term nudges $J$ towards zero row/column sums so that the random bulk stays centered. Our optimizer will be standard gradient descent through time (BPTT) on $J$ (and often the readout $w$, occasionally $B$). We can backpropagate the MSE through the RNN unrolled over time. We should monitor the spectrum of $J-I$ (or the linearized Jacobian) to see whether/when an outlier separates from the circular bulk. Additionally, some simple weight stats (row/col means, within/between clusters) to visualize emerging structure, also task error.

\textit{Goal}. The goal of this side project is to supplement the original DMFT analysis. Our current work analyzes fixed low-rank perturbations $J = gW - b/N \mathbf{1}\mathbf{1}^T + m uv^T$ and their dynmaical consequences. The training goal is to learn: if we \textit{do not} impose $muv^T$ structure, but instead train the RNN to do the OU-tracking task, does learning spontaneously add a low-rank component that looks like $uv^T$? How does the magnitude $m$ and alignment compare with the task geometry?

After training, we can project the learned change $\Delta J = J - J_0$ onto a low-rank model. We fit $\Delta J \approx \sum_{r=1}^R m_r u_r v_r^T$ (e.g. SVD with sign/scale conventions). Check whether $R = 1$ or $2$ suffices for most of the variance. We also should compare spectra: the bulk stays roughly circular, and we should see an outlier emerge and move with training, exactly the same object we analyzed in the DMFT (the $\mathfrak{R}\lambda_{\mathrm{out}}$ crossing).

\end{document}